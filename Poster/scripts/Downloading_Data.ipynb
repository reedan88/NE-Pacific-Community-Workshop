{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f51523",
   "metadata": {},
   "source": [
    "# Download Data\n",
    "\n",
    "### Purpose\n",
    "This jupyter notebook highlights two different methods for accessing and downloading data from Ocean Observatories Initiative Carbon System instruments. The first method utilizes OOI's API to perform M2M (Machine-2-Machine) queries for data from the OOI THREDDS data server. The second method requests data from OOI's DataExplorer ERDDAP server.\n",
    "\n",
    "#### THREDDs Data\n",
    "The data served up via OpenDAP on OOI THREDDs servers are the same datasets which can be accessed via OOI's Data Portal at https://ooinet.oceanobservatories.org/. This is the source for accessing realtime or near-realtime data from OOI. \n",
    "\n",
    "\n",
    "#### Data Explorer\n",
    "Data Explorer is the new tool for exploring, discovering, and downloading data from OOI. It can be accessed via the web at https://dataexplorer.oceanobservatories.org/. Data Explorer hosts \"gold copy\" versions of OOI datasets, with all the relevant data stream merged into a single unified file. These datasets are hosted on the Data Explorer ERDDAP server at  However, Data Explorer currently only from the Data Explorer website, they currently can't be downloaded from the ERDDAP server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c3c9c",
   "metadata": {},
   "source": [
    "---\n",
    "## OOINet/THREDDs\n",
    "First, we are going to access and download data from OOI's Data Portal. Then we will do some dataset reprocessing to make the resulting data easier and more intuitive to work with. This portion of the notebook relies on some community tools which have been developed by OOI's Data Team members which simplify interacting with OOI's API. The two tools are the OOINet tool (https://github.com/reedan88/OOINet) and the Data Explorations Modules (https://github.com/oceanobservatories/ooi-data-explorations).\n",
    "\n",
    "This notebook provides an example on how to use the OOINet download tool to perform the following functions:\n",
    "* Search for datasets\n",
    "* Identify desired reference designator\n",
    "* Get the associated metadata for a given reference designator\n",
    "* Request netCDF datasets for a reference designator\n",
    "* Download the netCDF dataset to your local machine\n",
    "\n",
    "The key parameters which the OOI API requires is the \"reference designator.\" A reference designator may be thought of as a type of instrument located at a fixed location and depth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed42fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os, shutil, sys, time, re, requests, csv, datetime, pytz\n",
    "import time\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d15f8509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OOINet M2M tool\n",
    "sys.path.append(\"../../M2M_tutorial/src/\")\n",
    "from pyOOI import M2M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a83ee2",
   "metadata": {},
   "source": [
    "---\n",
    "## Search Datasets\n",
    "First, we can search the available OOI Reference Designators (i.e. \"refdes\" for short) on the following keys: **array**, **node**, **instrument**. Additionally, can request for \"**English_names**\", which will return the descriptive name for the associated array, node, and instrument. Below, we will search for the available CTD instruments on the Pioneer Array Central Surface Mooring.\n",
    "\n",
    "The major caveat with the search is, similar to searching on ERDDAP datasets, the search terms must be partial or full match based on OOI nomenclature. For example, we have to search for \"PCO2\", \"PCO2AA\", or the full instrument name \"04-PCO2AA\" if we are searching for the sea-surface pCO2 sensor. We can't search \"pco2\", \"carbon dioxide\" or other instrument terms.\n",
    "\n",
    "gold_copy = 'http://thredds.dataexplorer.oceanobservatories.org/thredds/catalog/ooigoldcopy/public/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd06c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103a203166654f09b353f15b6de77a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>array</th>\n",
       "      <th>array_name</th>\n",
       "      <th>node</th>\n",
       "      <th>node_name</th>\n",
       "      <th>instrument</th>\n",
       "      <th>instrument_name</th>\n",
       "      <th>refdes</th>\n",
       "      <th>url</th>\n",
       "      <th>deployments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP02HYPM</td>\n",
       "      <td>Global Station Papa Apex Profiler Mooring</td>\n",
       "      <td>WFP03</td>\n",
       "      <td>Wire-Following Profiler Lower</td>\n",
       "      <td>05-VEL3DL000</td>\n",
       "      <td>3-D Single Point Velocity Meter</td>\n",
       "      <td>GP02HYPM-WFP03-05-VEL3DL000</td>\n",
       "      <td>https://ooinet.oceanobservatories.org/api/m2m/...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP02HYPM</td>\n",
       "      <td>Global Station Papa Apex Profiler Mooring</td>\n",
       "      <td>WFP03</td>\n",
       "      <td>Wire-Following Profiler Lower</td>\n",
       "      <td>04-CTDPFL000</td>\n",
       "      <td>CTD</td>\n",
       "      <td>GP02HYPM-WFP03-04-CTDPFL000</td>\n",
       "      <td>https://ooinet.oceanobservatories.org/api/m2m/...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP02HYPM</td>\n",
       "      <td>Global Station Papa Apex Profiler Mooring</td>\n",
       "      <td>WFP03</td>\n",
       "      <td>Wire-Following Profiler Lower</td>\n",
       "      <td>03-DOSTAL000</td>\n",
       "      <td>Dissolved Oxygen</td>\n",
       "      <td>GP02HYPM-WFP03-03-DOSTAL000</td>\n",
       "      <td>https://ooinet.oceanobservatories.org/api/m2m/...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP02HYPM</td>\n",
       "      <td>Global Station Papa Apex Profiler Mooring</td>\n",
       "      <td>WFP03</td>\n",
       "      <td>Wire-Following Profiler Lower</td>\n",
       "      <td>01-FLORDL000</td>\n",
       "      <td>2-Wavelength Fluorometer</td>\n",
       "      <td>GP02HYPM-WFP03-01-FLORDL000</td>\n",
       "      <td>https://ooinet.oceanobservatories.org/api/m2m/...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP02HYPM</td>\n",
       "      <td>Global Station Papa Apex Profiler Mooring</td>\n",
       "      <td>WFP03</td>\n",
       "      <td>Wire-Following Profiler Lower</td>\n",
       "      <td>00-WFPENG000</td>\n",
       "      <td>Profiler Controller</td>\n",
       "      <td>GP02HYPM-WFP03-00-WFPENG000</td>\n",
       "      <td>https://ooinet.oceanobservatories.org/api/m2m/...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GP02HYPM</td>\n",
       "      <td>Global Station Papa Apex Profiler Mooring</td>\n",
       "      <td>WFP02</td>\n",
       "      <td>Wire-Following Profiler Upper</td>\n",
       "      <td>05-VEL3DL000</td>\n",
       "      <td>3-D Single Point Velocity Meter</td>\n",
       "      <td>GP02HYPM-WFP02-05-VEL3DL000</td>\n",
       "      <td>https://ooinet.oceanobservatories.org/api/m2m/...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GP02HYPM</td>\n",
       "      <td>Global Station Papa Apex Profiler Mooring</td>\n",
       "      <td>WFP02</td>\n",
       "      <td>Wire-Following Profiler Upper</td>\n",
       "      <td>04-CTDPFL000</td>\n",
       "      <td>CTD</td>\n",
       "      <td>GP02HYPM-WFP02-04-CTDPFL000</td>\n",
       "      <td>https://ooinet.oceanobservatories.org/api/m2m/...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GP02HYPM</td>\n",
       "      <td>Global Station Papa Apex Profiler Mooring</td>\n",
       "      <td>WFP02</td>\n",
       "      <td>Wire-Following Profiler Upper</td>\n",
       "      <td>03-DOSTAL000</td>\n",
       "      <td>Dissolved Oxygen</td>\n",
       "      <td>GP02HYPM-WFP02-03-DOSTAL000</td>\n",
       "      <td>https://ooinet.oceanobservatories.org/api/m2m/...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GP02HYPM</td>\n",
       "      <td>Global Station Papa Apex Profiler Mooring</td>\n",
       "      <td>WFP02</td>\n",
       "      <td>Wire-Following Profiler Upper</td>\n",
       "      <td>01-FLORDL000</td>\n",
       "      <td>2-Wavelength Fluorometer</td>\n",
       "      <td>GP02HYPM-WFP02-01-FLORDL000</td>\n",
       "      <td>https://ooinet.oceanobservatories.org/api/m2m/...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GP02HYPM</td>\n",
       "      <td>Global Station Papa Apex Profiler Mooring</td>\n",
       "      <td>WFP02</td>\n",
       "      <td>Wire-Following Profiler Upper</td>\n",
       "      <td>00-WFPENG000</td>\n",
       "      <td>Profiler Controller</td>\n",
       "      <td>GP02HYPM-WFP02-00-WFPENG000</td>\n",
       "      <td>https://ooinet.oceanobservatories.org/api/m2m/...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GP02HYPM</td>\n",
       "      <td>Global Station Papa Apex Profiler Mooring</td>\n",
       "      <td>RIM01</td>\n",
       "      <td>Mooring Riser</td>\n",
       "      <td>02-CTDMOG039</td>\n",
       "      <td>CTD</td>\n",
       "      <td>GP02HYPM-RIM01-02-CTDMOG039</td>\n",
       "      <td>https://ooinet.oceanobservatories.org/api/m2m/...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GP02HYPM</td>\n",
       "      <td>Global Station Papa Apex Profiler Mooring</td>\n",
       "      <td>RIM01</td>\n",
       "      <td>Mooring Riser</td>\n",
       "      <td>00-SIOENG000</td>\n",
       "      <td>Platform Controller</td>\n",
       "      <td>GP02HYPM-RIM01-00-SIOENG000</td>\n",
       "      <td>https://ooinet.oceanobservatories.org/api/m2m/...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       array                                 array_name   node  \\\n",
       "0   GP02HYPM  Global Station Papa Apex Profiler Mooring  WFP03   \n",
       "1   GP02HYPM  Global Station Papa Apex Profiler Mooring  WFP03   \n",
       "2   GP02HYPM  Global Station Papa Apex Profiler Mooring  WFP03   \n",
       "3   GP02HYPM  Global Station Papa Apex Profiler Mooring  WFP03   \n",
       "4   GP02HYPM  Global Station Papa Apex Profiler Mooring  WFP03   \n",
       "5   GP02HYPM  Global Station Papa Apex Profiler Mooring  WFP02   \n",
       "6   GP02HYPM  Global Station Papa Apex Profiler Mooring  WFP02   \n",
       "7   GP02HYPM  Global Station Papa Apex Profiler Mooring  WFP02   \n",
       "8   GP02HYPM  Global Station Papa Apex Profiler Mooring  WFP02   \n",
       "9   GP02HYPM  Global Station Papa Apex Profiler Mooring  WFP02   \n",
       "10  GP02HYPM  Global Station Papa Apex Profiler Mooring  RIM01   \n",
       "11  GP02HYPM  Global Station Papa Apex Profiler Mooring  RIM01   \n",
       "\n",
       "                        node_name    instrument  \\\n",
       "0   Wire-Following Profiler Lower  05-VEL3DL000   \n",
       "1   Wire-Following Profiler Lower  04-CTDPFL000   \n",
       "2   Wire-Following Profiler Lower  03-DOSTAL000   \n",
       "3   Wire-Following Profiler Lower  01-FLORDL000   \n",
       "4   Wire-Following Profiler Lower  00-WFPENG000   \n",
       "5   Wire-Following Profiler Upper  05-VEL3DL000   \n",
       "6   Wire-Following Profiler Upper  04-CTDPFL000   \n",
       "7   Wire-Following Profiler Upper  03-DOSTAL000   \n",
       "8   Wire-Following Profiler Upper  01-FLORDL000   \n",
       "9   Wire-Following Profiler Upper  00-WFPENG000   \n",
       "10                  Mooring Riser  02-CTDMOG039   \n",
       "11                  Mooring Riser  00-SIOENG000   \n",
       "\n",
       "                    instrument_name                       refdes  \\\n",
       "0   3-D Single Point Velocity Meter  GP02HYPM-WFP03-05-VEL3DL000   \n",
       "1                               CTD  GP02HYPM-WFP03-04-CTDPFL000   \n",
       "2                  Dissolved Oxygen  GP02HYPM-WFP03-03-DOSTAL000   \n",
       "3          2-Wavelength Fluorometer  GP02HYPM-WFP03-01-FLORDL000   \n",
       "4               Profiler Controller  GP02HYPM-WFP03-00-WFPENG000   \n",
       "5   3-D Single Point Velocity Meter  GP02HYPM-WFP02-05-VEL3DL000   \n",
       "6                               CTD  GP02HYPM-WFP02-04-CTDPFL000   \n",
       "7                  Dissolved Oxygen  GP02HYPM-WFP02-03-DOSTAL000   \n",
       "8          2-Wavelength Fluorometer  GP02HYPM-WFP02-01-FLORDL000   \n",
       "9               Profiler Controller  GP02HYPM-WFP02-00-WFPENG000   \n",
       "10                              CTD  GP02HYPM-RIM01-02-CTDMOG039   \n",
       "11              Platform Controller  GP02HYPM-RIM01-00-SIOENG000   \n",
       "\n",
       "                                                  url  \\\n",
       "0   https://ooinet.oceanobservatories.org/api/m2m/...   \n",
       "1   https://ooinet.oceanobservatories.org/api/m2m/...   \n",
       "2   https://ooinet.oceanobservatories.org/api/m2m/...   \n",
       "3   https://ooinet.oceanobservatories.org/api/m2m/...   \n",
       "4   https://ooinet.oceanobservatories.org/api/m2m/...   \n",
       "5   https://ooinet.oceanobservatories.org/api/m2m/...   \n",
       "6   https://ooinet.oceanobservatories.org/api/m2m/...   \n",
       "7   https://ooinet.oceanobservatories.org/api/m2m/...   \n",
       "8   https://ooinet.oceanobservatories.org/api/m2m/...   \n",
       "9   https://ooinet.oceanobservatories.org/api/m2m/...   \n",
       "10  https://ooinet.oceanobservatories.org/api/m2m/...   \n",
       "11  https://ooinet.oceanobservatories.org/api/m2m/...   \n",
       "\n",
       "                    deployments  \n",
       "0   [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
       "1   [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
       "2   [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
       "3   [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
       "4   [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
       "5   [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
       "6   [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
       "7   [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
       "8   [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
       "9   [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
       "10  [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n",
       "11  [1, 2, 3, 4, 5, 6, 7, 8, 9]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruments = M2M.search_datasets(array=\"GP02HYPM\", English_names=True)\n",
    "instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f20b6e9",
   "metadata": {},
   "source": [
    "From the returned list of available instruments above, we can select a particular instrument using its **reference designator** (refdes for short):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ba746",
   "metadata": {},
   "outputs": [],
   "source": [
    "refdes = \"GP02HYPM-WFP02-05-VEL3DL000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f47be65",
   "metadata": {},
   "source": [
    "---\n",
    "## Metadata\n",
    "Next, we can query OOINet for the metadata associated with the selected reference designator. The metadata contains such valuable information such as the available methods and streams (which are required to download the data), the particleKeys (the data variable names), and the associated units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = M2M.get_metadata(refdes)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb59de",
   "metadata": {},
   "source": [
    "#### Sensor Parameters\n",
    "Each instrument returns multiple parameters containing a variety of low-level instrument output and metadata. However, we are interested in science-relevant parameters. We can identify the science parameters based on the preload database, which designates the science parameters with a \"data level\" of L1 or L2. \n",
    "\n",
    "Consequently, we will want to filter and group the metadata for a given reference designator to identify the relevant parameters. First, we query the preload database with the relevant metadata for a reference designator. Then, we filter the metadata for the science-relevant data streams based on the preload information. Then, we reduce the results by grouping by the stream parameter to get the stream-by-stream data, which will be useful when requesting data from OOINet for download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0bc96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_levels = M2M.get_parameter_data_levels(metadata)\n",
    "data_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88385ad",
   "metadata": {},
   "source": [
    "Filter the metadata based on the data levels for **L1** & **L2** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ed39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_parameter_ids(pdId, pid_dict):\n",
    "    data_level = pid_dict.get(pdId)\n",
    "    if data_level is not None:\n",
    "        if data_level > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e094827",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = metadata[\"pdId\"].apply(lambda x: filter_parameter_ids(x, data_levels))\n",
    "metadata = metadata[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f4091",
   "metadata": {},
   "source": [
    "Groupby based on the reference designator - method - stream to get the unique values for each data stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "metadata = metadata.reset_index()\n",
    "metadata = metadata.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a1e83",
   "metadata": {},
   "source": [
    "This returns all of the methods and streams which have scientific data. For PCO2W datasets, we want to drop the entries which have \"blank\" in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f9f3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = metadata[\"stream\"].apply(lambda x: False if \"blank\" in x else True)\n",
    "metadata = metadata[mask]\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1da7e",
   "metadata": {},
   "source": [
    "---\n",
    "## Deployment Information\n",
    "When we searched for datasets, it returned a table which listed the available deployment numbers for each of the datasets. We can get much more detailed information on the deployments for a particular reference designator by requesting the deployment information from OOINet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb82f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments = M2M.get_deployments(refdes=refdes)\n",
    "deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4e6c4",
   "metadata": {},
   "source": [
    "We'll go ahead and save the deployment data as a csv since it might be useful when working with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f103bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments.to_csv(f\"../data/{refdes}_deployments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441fab9d",
   "metadata": {},
   "source": [
    "---\n",
    "## Vocab Information\n",
    "Additionally, if we are interested in more detailed information on the location that the reference designator is assigned to, we can request the vocab information for the given reference designator. The vocab information includes some of the \"**English_names**\" info we requested when searching for datasets, as well as instrument model, manufacturer, and the descriptive names for the reference designator location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9810c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = M2M.get_vocab(refdes=refdes)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc162239",
   "metadata": {},
   "source": [
    "---\n",
    "## Calibration Information\n",
    "We can also request the calibration information for a given reference designator. Since individual instruments are swapped during each mooring deployment & recovery, the calibration coefficients for a reference designator are different for each deployment. The way OOI operates is that it loads all the available calibration coefficients for a given reference designator. Then, for each deployment, it finds the calibration coefficients with the most recent calibration date which most closely _precedes_ the start of the deployment. The result is a table, sorted by deployment number for a reference designator, with the uid of the specific instrument, its calibration coefficients, when the instrument was calibrated, and the source of the calibration coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrations = M2M.get_calibrations(refdes, deployments)\n",
    "calibrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a9aa2",
   "metadata": {},
   "source": [
    "It is also possible to request the calibration history for a specific instrument by utilizing the **uid** of the instrument and using the lower-level ```_get_api``` method and ```OOINet.URLS``` attribute to construct your own request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the calibration url and arguments to pass to the request\n",
    "cal_url = OOINet.URLS[\"cal\"]\n",
    "uid = \"CGINS-PHSENF-P0183\" # This is unique to each instrument\n",
    "params = {\n",
    "    \"uid\": uid\n",
    "}\n",
    "\n",
    "# Make the request\n",
    "calInfo = OOINet._get_api(cal_url, params=params)\n",
    "\n",
    "# Put the data into a pandas dataframe, sorted by calibration date and coefficient name\n",
    "columns = [\"uid\", \"calCoef\", \"calDate\", \"value\", \"calFile\"]\n",
    "instrumentCals = pd.DataFrame(columns=columns)\n",
    "for c in calInfo[\"calibration\"]:\n",
    "    for cc in c[\"calData\"]:\n",
    "        instrumentCals = instrumentCals.append({\n",
    "            \"uid\": cc[\"assetUid\"],\n",
    "            \"calCoef\": cc[\"eventName\"],\n",
    "            \"calDate\": OOINet._convert_time(cc[\"eventStartTime\"]),\n",
    "            \"value\": cc[\"value\"],\n",
    "            \"calFile\": cc[\"dataSource\"]\n",
    "        }, ignore_index=True)\n",
    "instrumentCals.sort_values(by=[\"calDate\", \"calCoef\"], inplace=True)\n",
    "instrumentCals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a2afa2",
   "metadata": {},
   "source": [
    "---\n",
    "## Download Datasets\n",
    "The ultimate goal of the queries above were to identify what data streams(s) we are interested in, along with supporting metadata/calibration information, in order to request the to download. Now we want to be able to request those data streams and get the associated netCDF files. This process involves the following steps:\n",
    "1. Identify the methods and data streams for the selected reference designator\n",
    "2. Request the THREDDS server url for the data sets\n",
    "3. Get the catalog of datasets on the THREDDS server\n",
    "4. Parse the catalog for the desired netCDF files\n",
    "5. Download the identified netCDF files to a local directory\n",
    "\n",
    "Below, we script the above steps in order to download all of the available datasets. In the following section we will combine the data delivered via different methods (e.g. telemetered, recovered_host, recovered_inst) to generate a single combined dataset with the most complete data record available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_catalog(catalog, stream, deployments):\n",
    "    \"\"\"Clean up the netCDF catalog of unwanted datasets\"\"\"\n",
    "    # Parse the netCDF datasets to only get those with the datastream in its name\n",
    "    datasets = []\n",
    "    for dset in catalog:\n",
    "        check = dset.split(\"/\")[-1]\n",
    "        if stream in check:\n",
    "            datasets.append(dset)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # Next, check that the netCDF datasets are not empty by getting the timestamps in the\n",
    "    # datasets and checking if they are \n",
    "    catalog = datasets\n",
    "    datasets = []\n",
    "    for dset in catalog:\n",
    "        # Get the timestamps\n",
    "        timestamps = dset.split(\"_\")[-1].replace(\".nc\",\"\").split(\"-\")\n",
    "        t1, t2 = timestamps\n",
    "        # Check if the timestamps are equal\n",
    "        if t1 == t2:\n",
    "            pass\n",
    "        else:\n",
    "            datasets.append(dset)\n",
    "            \n",
    "    # Next, determine if the dataset is either for the given instrument\n",
    "    # or an ancillary instrument which supplies and input variable\n",
    "    catalog = datasets\n",
    "    datasets = []\n",
    "    ancillary = []\n",
    "    for dset in catalog:\n",
    "        if re.search(stream, dset.split(\"/\")[-1]) is None:\n",
    "            ancillary.append(dset)\n",
    "        else:\n",
    "            datasets.append(dset)\n",
    "            \n",
    "    # Finally, check that deployment numbers match what is in deployments metadata\n",
    "    catalog = datasets\n",
    "    datasets = []\n",
    "    for dset in catalog:\n",
    "        dep = re.findall(\"deployment[\\d]{4}\", dset)[0]\n",
    "        depNum = int(dep[-4:])\n",
    "        if depNum not in list(deployments[\"deploymentNumber\"]):\n",
    "            pass\n",
    "        else:\n",
    "            datasets.append(dset)\n",
    "            \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469f731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in metadata.index:\n",
    "    # Get the method and stream\n",
    "    method, stream = metadata.loc[row,\"method\"], metadata.loc[row, \"stream\"]\n",
    "    \n",
    "    if \"air\" in stream:\n",
    "        continue\n",
    "    \n",
    "   \n",
    "    # Get the THREDDS url\n",
    "    thredds_url = M2M.get_thredds_url(refdes, method, stream, goldCopy=True)\n",
    "    \n",
    "    # Get the catalog\n",
    "    catalog = M2M.get_thredds_catalog(thredds_url)\n",
    "    \n",
    "    # Clean the catalog\n",
    "    catalog = clean_catalog(catalog, stream, deployments)\n",
    "    \n",
    "    # Remove unwanted datasets from the catalog\n",
    "    for dataset in catalog:\n",
    "        if \"blank\" in dataset:\n",
    "            catalog.remove(dataset)\n",
    "    \n",
    "    # Create a directory to save the data\n",
    "    save_dir = f\"../data/{refdes}/{method}/\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Download the files to the save directory\n",
    "    M2M.download_netCDF_files(catalog, goldCopy=True, saveDir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a187f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "refdes, method, stream, save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the PCO2A, need to get just the \"water\" stream\n",
    "os.listdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d83bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3140cb7",
   "metadata": {},
   "source": [
    "### Merge Datasets\n",
    "\n",
    "With the datasets downloaded to a local directory, we now want to combine the datasets delivered via the different methods into a single dataset. This dataset should have the most complete data record available for the given reference designator.\n",
    "\n",
    "\n",
    "#### Load the data\n",
    "First, load the downloaded data into xarray datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1cdf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the telemetered data sets\n",
    "telemetered_files = os.listdir(f\"../data/{refdes}/telemetered\")\n",
    "telemetered_files = sorted([f\"../data/{refdes}/telemetered/\" + f for f in telemetered_files if \"metbk\" not in f])\n",
    "telemetered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the PCO2A, need just the \"water\" data stream\n",
    "remove_files = []\n",
    "for file in telemetered_files:\n",
    "    if \"air\" in file:\n",
    "        remove_files.append(file)\n",
    "\n",
    "for f in remove_files:\n",
    "    telemetered_files.remove(f)\n",
    "    \n",
    "telemetered_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba8c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the recovered_host data sets\n",
    "recovered_host_files = os.listdir(f\"../data/{refdes}/recovered_host\")\n",
    "recovered_host_files = sorted([f\"../data/{refdes}/recovered_host/\" + f for f in recovered_host_files if \"metbk\" not in f])\n",
    "recovered_host_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6284661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the PCO2A, need just the \"water\" data stream\n",
    "remove_files = []\n",
    "for file in recovered_host_files:\n",
    "    if \"air\" in file:\n",
    "        remove_files.append(file)\n",
    "\n",
    "for f in remove_files:\n",
    "    recovered_host_files.remove(f)\n",
    "    \n",
    "recovered_host_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the recovered_inst data sets\n",
    "recovered_inst_files = os.listdir(f\"../data/{refdes}/recovered_inst\")\n",
    "recovered_inst_files = sorted([f\"../data/{refdes}/recovered_inst/\" + f for f in recovered_inst_files if \"metbk\" not in f])\n",
    "\n",
    "recovered_inst_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47619848",
   "metadata": {},
   "source": [
    "Load the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd8950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refdes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_datasets(datasets, refdes):\n",
    "    \"\"\"Opens datasets saved locally into an xarray dataset.\"\"\"\n",
    "    \n",
    "    M2M.REFDES = refdes\n",
    "    \n",
    "    # Clean the catalog\n",
    "    \n",
    "    \n",
    "    # Load the datasets into a concatenated xarray DataSet\n",
    "    with ProgressBar():\n",
    "        print(\"\\n\"+f\"Loading netCDF_files for {M2M.REFDES}:\")\n",
    "        ds = xr.open_mfdataset(datasets, preprocess=OOINet._preprocess, parallel=True)\n",
    "        \n",
    "    # Add in the English name of the dataset\n",
    "    refdes = \"-\".join(ds.attrs[\"id\"].split(\"-\")[:4])\n",
    "    vocab = M2M.get_vocab(refdes)\n",
    "    ds.attrs[\"Location_name\"] = \" \".join((vocab[\"tocL1\"].iloc[0],\n",
    "                                          vocab[\"tocL2\"].iloc[0],\n",
    "                                          vocab[\"tocL3\"].iloc[0]))    \n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf979eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "refdes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tele_data = open_datasets(telemetered_files, refdes)\n",
    "host_data = open_datasets(recovered_host_files, refdes)\n",
    "inst_data = open_datasets(recovered_inst_files, refdes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b907b7",
   "metadata": {},
   "source": [
    "#### Optional Step: Process the dataset\n",
    "An additional step is to process the datasets to clean up the datasets and get rid of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/areed/Documents/OOI/oceanobservatories/ooi-data-explorations/python/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67dbe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.uncabled import process_ctdbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_data = process_ctdbp.ctdbp_instrument(inst_data)\n",
    "host_data = process_ctdbp.ctdbp_datalogger(host_data)\n",
    "tele_data = process_ctdbp.ctdbp_datalogger(tele_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b2a63",
   "metadata": {},
   "source": [
    "#### Merge data\n",
    "Now, we need to merge the data. First, we iterate through the data variables for each dataset, identify any which are unique to a given dataset, and broadcast them to the other datasets. This step is necessary to allow the datasets to combine. Once each dataset has the same data variables, we utilize ```xr.combine_first``` to combine the datasets. We assume that the instrument record, if available, is the best and most complete dataset and utilize the telemetered and recovered_host datasets to fill in the gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make sure each dataset has the same variables\n",
    "for var in tele_data.variables:\n",
    "    if var not in host_data.variables:\n",
    "        host_data[var] = tele_data[var].broadcast_like(host_data[\"time\"])\n",
    "        \n",
    "for var in host_data.variables:\n",
    "    if var not in tele_data.variables:\n",
    "        tele_data[var] = host_data[var].broadcast_like(tele_data[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abe482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the telemetered dataset and host_dataset\n",
    "tele_host = tele_data.combine_first(host_data)\n",
    "data = tele_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14fbb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in tele_host.variables:\n",
    "    if var not in inst_data.variables:\n",
    "        inst_data[var] = tele_host[var].broadcast_like(inst_data[\"time\"])\n",
    "\n",
    "for var in inst_data.variables:\n",
    "    if var not in tele_host.variables:\n",
    "        tele_host[var] = inst_data[var].broadcast_like(tele_host[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5943734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the instrument dataset with the combined telemetered-recovered_host dataset\n",
    "data = inst_data.combine_first(tele_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f915b4a",
   "metadata": {},
   "source": [
    "#### Save the results\n",
    "With the merged datasets, we can save the results locally as a netCDF file. However, some data variables contain improperly formatted datetimes and timestamps which will cause an error when saving. Generally, these data variables do not contain particularly useful information for a science-user and can be dropped before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ed7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "refdes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5056e2",
   "metadata": {},
   "source": [
    "Save the data as a netCDF file using h5netcdf compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_netcdf(f\"../data/{refdes}_combined.nc\", engine=\"h5netcdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2237a076",
   "metadata": {},
   "source": [
    "Close the dataset so it can be operated on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dabcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8fc18",
   "metadata": {},
   "source": [
    "---\n",
    "## Annotations\n",
    "Annotations contain important qualitative assessments of data quality from the instrument operators. They may range from explanations for why data is missing for a given time period to information about biofouling or other data quality issues. Annotations can be downloaded from OOINet for a particular reference designator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the annotations for each reference designator\n",
    "annotations = OOINet.get_annotations(refdes)\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f259695",
   "metadata": {},
   "source": [
    "Save the annotations to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae63acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.to_csv(f\"../data/{refdes}_annotations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fccdd9",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Explorer\n",
    "---\n",
    "The data from Data Explorer are hosted via ERDDAP. To interact with Data Explorer's ERDDAP, we'll utilize the python package ```erddapy```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bda6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from erddapy import ERDDAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf14192",
   "metadata": {},
   "source": [
    "**Data Explorer ERDDAP url**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataExplorer = \"http://erddap.dataexplorer.oceanobservatories.org/erddap\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378ba30",
   "metadata": {},
   "source": [
    "Connect to the Data Explorer ERDDAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "erd = ERDDAP(server=dataExplorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25c199",
   "metadata": {},
   "source": [
    "Search for ```PHSEN``` on the Irminger Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c62631",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = erd.get_search_url(search_for=\"gi03flma phsen\", \n",
    "                                protocol=\"tabledap\",\n",
    "                                response=\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336b40d",
   "metadata": {},
   "source": [
    "Get the dataset ids for the available PHSEN datasets on the Irminger Sea Flanking Mooring A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ids = pd.read_csv(search_url)[\"Dataset ID\"]\n",
    "dataset_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332df18",
   "metadata": {},
   "source": [
    "Download the dataset from ERDDAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428eaec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dataset id of the instrument you want to download\n",
    "dataset_id = \"ooi-gi03flma-ris01-04-phsenf000\"\n",
    "\n",
    "# Get the download url\n",
    "download_url = erd.get_download_url(dataset_id=dataset_id, \n",
    "                                    protocol=\"tabledap\",\n",
    "                                    response=\"opendap\")\n",
    "\n",
    "# Set up the parameters for the dataset request from the ERDDAP server\n",
    "erd.dataset_id = dataset_id\n",
    "erd.response = \"nc\"\n",
    "erd.protocol = \"tabledap\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9112a91",
   "metadata": {},
   "source": [
    "Open the requested dataset using ```xarray```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1fa83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = erd.to_xarray()\n",
    "ds = ds.swap_dims({\"obs\":\"time\"})\n",
    "ds = ds.sortby(\"time\")\n",
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
