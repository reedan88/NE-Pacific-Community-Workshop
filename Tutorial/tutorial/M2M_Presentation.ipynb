{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff62838e",
   "metadata": {},
   "source": [
    "# Using M2M: Programmatic Interaction with OOI Data & Metadata\n",
    "Author: Andrew Reed\n",
    "\n",
    "### Purpose\n",
    "This notebook is the companion to the presentation \"**Using M2M: Programmatic Interaction with OOI Data & Metadata**\" presented as part of the Ocean Observatories Facilities Board **NE Pacific Community Workshop** in Portland, Oregon, from June 7 - 9, 2022. The goal is to walk through how to use the Ocean Observatories Initiative's API, named Machine-2-Machine (M2M), system and examine what data can be queried and how to manipulate it to extract desired information. The table below outlines the different categories of data which can be queried, their access points in the API, and a short description.\n",
    "\n",
    "| Category | Access Point | Description |\n",
    "| -------- | ------------ | ----------- |\n",
    "| Deployment | 12587/events/deployment/inv/ | Access deployment numbers as well as the asset & calibration info for specified instrument & deployment, and deployment times/cruises |\n",
    "| Deployment | 12587/asset/deployments | Asset & calibration info for all deployment for the specified UID |\n",
    "| Calibration | /12587/asset/cal?uid= OR ?assetid= | Return all calibration info for a given uid or assetId |\n",
    "| Calibration | /12587/asset/cal?refdes= | Return list of deployments with calibrations for a given reference designator |\n",
    "| Asset | 12587/asset?uid= OR ?serialnumber= | Asset information by unique id or instrument serial number |\n",
    "| Preload | 12575/parameter/ | Retrieve information for a parameter (i.e. variable) given its ID number |\n",
    "| Preload | 12575/stream/byname/ | Retrieve information for a stream given its name |\n",
    "| Annotations | /12580/anno/find?= | Retrieve annotations for a specific time period and for a given reference designator (optional: stream and method) |\n",
    "| Vocab | 12586/vocab/inv/ | Get the vocabulary (descriptions) for a sensor |\n",
    "| Data | 12576/sensor/inv/ | Can access the data from OOI using either a synchronous (returns JSON; limited to 20000 data points) or asynchronous (returns netCDF, CSV, or JSON; not data limit) |\n",
    "\n",
    "The tutorial and presentation are based off of similar work developed by Sage Lichtenwalder (github: @seagrinch) for the 2018 OOI Data Workshops. \n",
    "\n",
    "### Setup\n",
    "First, please go to https://ooinet.oceanobservatories.org/ and make an account for yourself. Once you have registered and logged in, navigate to your account settings by clicking on \"User Profile\" under your email in the top right corner of your screen. Once at your Profile, record your API Username and API Token. These are necessary if you wish to access and download data from the Ocean Observatories API.\n",
    "\n",
    "Additionally, this notebook makes use of the code contained in the partner package pyOOI. A stripped-down version of this package has been included as a module in the repository with this tutorial to allow for direct import. Further code scripts and functions may also be found on github.com/oceanobservatories/ooi-data-explorations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2de757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import yaml\n",
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacdc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ac85ee",
   "metadata": {},
   "source": [
    "Import the M2M module from the pyOOI package if it is downloaded locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../src/\")\n",
    "from pyOOI.M2M import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99482a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in URLS.keys():\n",
    "    dtype = key\n",
    "    url = URLS.get(dtype)\n",
    "    print(dtype + \" :: \" + url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daebe84",
   "metadata": {},
   "source": [
    "---\n",
    "## Navigating the API\n",
    "Navigating the OOI M2M end-points can be confusing. We can reference the helpful OOI API cheat-sheet. There are also several other quirks with how OOI delivers data. First, some queries to OOI will return . These are some basic functions needed to interoperate with the OOI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d058c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntp_seconds_to_datetime(ntp_seconds):\n",
    "    \"\"\"Convert OOINet timestamps to unix-convertable timestamps.\"\"\"\n",
    "    # Specify some constant needed for timestamp conversions\n",
    "    ntp_epoch = datetime.datetime(1900, 1, 1)\n",
    "    unix_epoch = datetime.datetime(1970, 1, 1)\n",
    "    ntp_delta = (unix_epoch - ntp_epoch).total_seconds()\n",
    "\n",
    "    return datetime.datetime.utcfromtimestamp(ntp_seconds - ntp_delta)\n",
    "\n",
    "def convert_time(ms):\n",
    "    if ms is None:\n",
    "        return None\n",
    "    else:\n",
    "        return datetime.datetime.utcfromtimestamp(ms/1000)\n",
    "\n",
    "def unix_epoch_time(date_time):\n",
    "    \"\"\"Convert a datetime to unix epoch microseconds.\"\"\"\n",
    "    # Convert the date time to a string\n",
    "    date_time = int(pd.to_datetime(date_time).strftime(\"%s\"))*1000\n",
    "    return date_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89165a55",
   "metadata": {},
   "source": [
    "---\n",
    "## Finding Data\n",
    "\n",
    "The first step in downloading data from the OOI M2M is to find the datasets that you want to download. We can do this by querying the \"data\" API through its various endpoints until we have the sensor that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f2c84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Start with the basic - just requesting the very basic gives you a list of the available sites - OOI parlance\n",
    "# for the different moorings (i.e. Global Station Papa Flanking Mooring A = GP03FLMA)\n",
    "print(\"API Endpoint: \" + URLS[\"data\"] + \"\\n\")\n",
    "\n",
    "print(\"Returns the following list of sites: \\n\")\n",
    "sites = get_api(URLS[\"data\"])\n",
    "print(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a site: Global Flanking Mooring A\n",
    "site = \"GP03FLMA\"\n",
    "\n",
    "# Can further narrow down the search - adding in the site will generate a list of the \"nodes\" on the mooring\n",
    "print(\"API Endpoint: \" + URLS[\"data\"] + \"/\" + site + \"\\n\")\n",
    "\n",
    "print(f\"Returns the following list of nodes on {site}: \\n\")\n",
    "nodes = get_api(URLS[\"data\"] + \"/\" + site)\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8be14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a node - in this case the Mooring Riser\n",
    "node = \"RIS01\"\n",
    "\n",
    "# Can further narrow the search - adding in the node with generate a list of the sensors on the given platform and node\n",
    "print(\"API Endpoint: \" + URLS[\"data\"] + \"/\" + site + \"/\" + node + \"\\n\")\n",
    "\n",
    "# Next, we can get all of the sensors on a given mooring node\n",
    "print(f\"Returns the following list of sensors on {site}-{node}: \\n\")\n",
    "sensors = get_api(URLS[\"data\"] + \"/\" + site + \"/\" + node)\n",
    "print(sensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa7d338",
   "metadata": {},
   "source": [
    "Here we can see there are four sensors on the Global Flanking Mooring A - Riser. One of the sensors (00-SIOENG000) returns engineering/operations data and does not have useful science data. The other three sensors are:\n",
    "* 03-DOSTAD000 - oxygen sensor\n",
    "* 04-PHSENF000 - pH sensor\n",
    "* 05-FLORTD000 - chlorophyll/turbidity sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bae216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sensor - in this case the dissolved oxygen sensor\n",
    "sensor = \"03-DOSTAD000\"\n",
    "\n",
    "# With the site-node-sensors we can construct a \"reference designator\" or refdes for short\n",
    "print(f\"Site: {site}\")\n",
    "print(f\"Node: {node}\")\n",
    "print(f\"Sensor: {sensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4e8424",
   "metadata": {},
   "source": [
    "With the **site**, **node**, and **sensor** we can construct the **reference designator** or **refdes** for short. The **reference designator** identifies a particular instrument that has been deployed as part of a site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b699e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "refdes = \"-\".join((site, node, sensor))\n",
    "print(f\"Reference Designator: {refdes}\")\n",
    "print(f\"Site: {site}\")\n",
    "print(f\"Node: {node}\")\n",
    "print(f\"Sensor: {sensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdcbd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# With a sensor selected, we can see what data delivery methods are available\n",
    "print(\"API Endpoint: \" + URLS[\"data\"] + \"/\" + site + \"/\" + node + \"/\" + sensor + \"\\n\")\n",
    "\n",
    "# Next, we can get all of the sensors on a given mooring node\n",
    "print(f\"Returns the following list of data delivery methods on {site}-{node}-{sensor}: \\n\")\n",
    "methods = get_api(URLS[\"data\"] + \"/\" + site + \"/\" + node + \"/\" + sensor)\n",
    "print(methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8768e043",
   "metadata": {},
   "source": [
    "The **Data Delivery Method** specifies how the data was either transmitted or recorded. In this case, we have the options:\n",
    "* **recovered_host**: Data downloaded directly from the computer on the mooring or asset which logs the data from the attached instruments\n",
    "* **telemetered**: Data received through wireless transmission, e.g. surface buoy to satellite, glider to satellite, etc. Telemetered data is frequently truncated or decimated to reduce size for transmission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a data delivery method\n",
    "method = \"recovered_host\"\n",
    "\n",
    "# Can further narrow the search - adding in the node with generate a list of the sensors on the given platform and node\n",
    "print(\"API Endpoint: \" + URLS[\"data\"] + \"/\" + site + \"/\" + node + \"/\" + sensor + \"/\" + method + \"\\n\")\n",
    "\n",
    "# Next, we can get all of the sensors on a given mooring node\n",
    "print(f\"Returns the following list of streams for {site}-{node}-{sensor} {method}: \\n\")\n",
    "sensors = get_api(URLS[\"data\"] + \"/\" + site + \"/\" + node + \"/\" + sensor + \"/\" + method)\n",
    "print(sensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef5e85",
   "metadata": {},
   "source": [
    "The **Data Streams** are generated from parsing the sensor raw data and separating it based on content (e.g. science, engineering, metadata, etc.). In this case, we have the options:\n",
    "* **dosta_abcdjm_sio_metadata_recovered**: this is the stream which contains metadata and sensor engineering data\n",
    "* **dosta_abcdjm_sio_instrument_recovered**: this stream contains the science-relevant data we are interested in getting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the stream\n",
    "stream = \"dosta_abcdjm_sio_instrument_recovered\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87919ff",
   "metadata": {},
   "source": [
    "#### Search Datasets\n",
    "\n",
    "For your convience, the function ```search_datasets``` included in the tutorial package can search the available OOI Reference Designators (i.e. \"refdes\" for short) on the following keys: **array**, **node**, **instrument**. Additionally, can request for \"**English_names**\", which will return the descriptive name for the associated array, node, and instrument. The function uses the knowledge of the sensor endpoints outlined above to crawl through the endpoints looking for available datasets which fit the search keys. Below, we will search for the available CTD instruments on the Global Ocean Station Papa Flanking Mooring A. Adding the \"**English_names**\" make use of the **vocab** url which we'll explore in a section lower-down in this notebook.\n",
    "\n",
    "The major caveat with the search is, similar to when searching on ERDDAP datasets, the search terms must be partial or full match based on OOI nomenclature. For example, if we were looking for CTDs, we would have to search for \"CTD\", \"CTDMO\", or the full instrument name \"02-CTDMOH051\". We can't search \"conductivity\", \"temperature\" or other CTD-related instrument terms.\n",
    "\n",
    "We'll search the Global Ocean Station Papa Flanking Mooring A Datasets for any oxygen sensors, all of which will start with \"DO\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d63fd3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "papa_datasets = search_datasets(array=\"GP03FLMA\", instrument=\"DO\", English_names=True)\n",
    "papa_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc455bbd",
   "metadata": {},
   "source": [
    "You will still need to query the M2M API to get the available methods and data streams for the reference designator that you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b32ed",
   "metadata": {},
   "source": [
    "Now, we could go ahead and request data for the Global Ocean Station Papa Mooring Riser Dissolved Oxygen Sensor by using the API Endpoint https://ooinet.oceanobservatories.org/api/m2m/12576/sensor/inv/GP03FLMA/RIS01/03-DOSTAD000/recovered_host/dosta_abcdjm_sio_instrument_recovered. However, this request will return _all_ of the available data and _all_ of the parameters for the sensor, including a lot of engineering or unprocessed data.\n",
    "\n",
    "Instead, we can interrogate the OOI M2M system to get information on when the sensor has been deployed, what parameters are available on what data streams, and start to narrow our data request to only those parameters, time periods, and/or deployments that we may be interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af29c6f",
   "metadata": {},
   "source": [
    "---\n",
    "## Deployment Information\n",
    "A deployment is defined as the span of time a mooring or instrument were deployed and then recovered. When we searched for the dissolved oxygen sensor on the Global Ocean Station Papa Flanking Mooring A, it returned a table which listed the available deployment numbers for each of the datasets. We can get much more detailed information on the deployments for a particular reference designator by requesting the deployment information from OOINet. \n",
    "\n",
    "We can start by using the deployment endpoint and reference designator to get a list of the available deployments for the reference designator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refdes = \"GP03FLMA-RIS01-03-DOSTAD000\"\n",
    "site, node, sensor = refdes.split(\"-\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97939df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, we can request the deployment numbers for the given site-node-stream\n",
    "print(\"API Endpoint: \" + \"/\".join((URLS[\"deploy\"], site, node, sensor)) + \"\\n\")\n",
    "print(get_api(\"/\".join((URLS[\"deploy\"], site, node, sensor))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b677c",
   "metadata": {},
   "source": [
    "If we want more detailed information about a given deployment, we need to add in the deployment number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266fc26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns asset and calibration information for deployment\n",
    "deployment_number = \"5\"\n",
    "data = get_api(\"/\".join((URLS[\"deploy\"], site, node, sensor, deployment_number)))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce66dd4e",
   "metadata": {},
   "source": [
    "This returns a json object with a dictionary that contains a lot of very, very detailed information. Depending on what you want, this has to be parsed out. Since we are interested in deployment information, lets\n",
    "parse that relevant info such as deployment start/end times, the unique ID of the instrument deployed, what cruise it was deployed on, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploymentInfo = pd.DataFrame()\n",
    "for d in data: # If you requested more than one deployment\n",
    "    deploymentInfo = deploymentInfo.append( {\n",
    "        \"deploymentNumber\": d.get(\"deploymentNumber\"),\n",
    "        \"referenceDesignator\": d.get(\"referenceDesignator\"),\n",
    "        \"mooring\": d.get(\"mooring\").get(\"description\"),\n",
    "        \"sensor\": d.get(\"sensor\").get(\"description\"),\n",
    "        \"UID\": d.get(\"sensor\").get(\"uid\"),\n",
    "        \"deployDateTime\": convert_time(d.get(\"eventStartTime\")).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"recoverDateTime\": convert_time(d.get(\"eventStopTime\")).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"deployCruise\": d.get(\"deployCruiseInfo\").get(\"eventName\"),\n",
    "        \"recoverCruise\": d.get(\"recoverCruiseInfo\").get(\"eventName\")\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "deploymentInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e312d4",
   "metadata": {},
   "source": [
    "The included function in the tutorial package ```get_deployments``` takes a reference designator, i.e. GP03FLMA-RIS01-02-DOSTAD000, parses it into the {site}/{node}/{sensor} information, and fetches all of the available info on the deployments for the given reference designator and returns it as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8d049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deployments = get_deployments(refdes)\n",
    "deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c66a6b",
   "metadata": {},
   "source": [
    "---\n",
    "## Vocab Information\n",
    "Additionally, if we are interested in more detailed information on the location that the reference designator is assigned to, we can request the vocab information for the given reference designator. The request returns a JSON object with details on the instrument such as the descriptive names for the reference designator location, the nominal depths, the manufacturer as well as the instrument model, etc. The vocab information includes some of the \"**English_names**\" info we requested when searching for datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f4b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_url = \"/\".join((URLS[\"vocab\"], site, node, sensor))\n",
    "print(vocab_url + \"\\n\") \n",
    "\n",
    "vocab = get_api(vocab_url)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f912cb8",
   "metadata": {},
   "source": [
    "The included function ```get_vocab``` performs the request and reformats the JSON object into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f3634",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(refdes)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1d2fa",
   "metadata": {},
   "source": [
    "---\n",
    "## Calibration Information\n",
    "We can also request the calibration information for a given reference designator. Since individual instruments are swapped during each mooring deployment & recovery, the calibration coefficients for a reference designator are different for each deployment. The way OOI operates is that it loads all the available calibration coefficients for a given reference designator. Then, for each deployment, it finds the calibration coefficients with the most recent calibration date which most closely _precedes_ the start of the deployment. The result is a table, sorted by deployment number for a reference designator, with the uid of the specific instrument, its calibration coefficients, when the instrument was calibrated, and the source of the calibration coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750716f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_url = URLS[\"cal\"] + \"?refdes=\" + refdes\n",
    "print(cal_url + \"\\n\")\n",
    "\n",
    "cal_data = get_api(cal_url)\n",
    "cal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c814b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b43ce91",
   "metadata": {},
   "source": [
    "We can see that the returned JSON object has multiple entries. Since individual instruments are swapped during each mooring deployment & recovery, the calibration coefficients for a reference designator are different for each deployment. The way OOI operates is that it loads all the available calibration coefficients for a given reference designator. Then, for each deployment, it finds the calibration coefficients with the most recent calibration date which most closely _precedes_ the start of the deployment, and applies those for the given deployment. \n",
    "\n",
    "However, when going through each entry, you'll notice that each deployment entry has all of the calibrations in the system entered for that given instrument UID. This makes parsing it very confusing. A better approach is to limit your request to a single deployment or even a single day by adding in **beginDT** and **endDT** to the request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed866e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Limit the request to Deployment 6: summer of 2018 to summer of 2019\n",
    "beginDT = \"2018-08-29T22:54:00.000Z\"\n",
    "endDT = \"2018-08-30T22:54:00.000Z\"\n",
    "\n",
    "# \n",
    "cal_url = URLS[\"cal\"] + \"?refdes=\" + refdes + \"&beginDT=\" + beginDT + \"&endDT=\" + endDT\n",
    "print(\"API Endpoint: \" + cal_url + \"\\n\")\n",
    "\n",
    "cal_data = get_api(cal_url)\n",
    "cal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106fe71",
   "metadata": {},
   "source": [
    "Now we can parse the JSON file to get the relevant calibration information for the DOSTA instrument deployed at Global Ocean Station Papa for deployment number 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4ecd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calibrations = pd.DataFrame()\n",
    "for c in cal_data:\n",
    "    deploymentNumber = c.get(\"deploymentNumber\")\n",
    "    uid = c.get(\"sensor\").get(\"uid\")\n",
    "    for cc in c.get(\"sensor\").get(\"calibration\"):\n",
    "        for ccc in cc.get(\"calData\"):\n",
    "            name = ccc.get(\"eventName\")\n",
    "            value = ccc.get(\"value\")\n",
    "            source = ccc.get(\"dataSource\")\n",
    "            # Update the calibration data frame\n",
    "            calibrations = calibrations.append({\n",
    "                \"deploymentNumber\": deploymentNumber,\n",
    "                \"uid\": uid,\n",
    "                \"calCoef\": name,\n",
    "                \"value\": value,\n",
    "                \"calFile\": source,\n",
    "            }, ignore_index=True)\n",
    "            \n",
    "calibrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0880019",
   "metadata": {},
   "source": [
    "The function ```get_calibrations``` included in this tutorial drastically simplifies the requests. However, it does need the **deployment** information fetched using the ```get_deployments``` function. What is nice about the function is that it will return the calibrations applicable for each deployment and just that deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1431f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrations = get_calibrations_by_refdes(refdes, deployments)\n",
    "calibrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eccd6b",
   "metadata": {},
   "source": [
    "It is much easier to query for calibration data by the UID, or unique ID, for an instrument. For example, if we are looking at the oxygen optode deployed for Deployment 6, it has a UID of CGINS-DOSTAD-00228. We can request the calibration coefficients for that particular instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a570f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the calibration url and arguments to pass to the request\n",
    "uid = \"CGINS-DOSTAD-00228\" # This is unique to each instrument\n",
    "cal_url = URLS[\"cal\"] + \"?uid=\" + uid\n",
    "print(\"API Endpoint: \" + cal_url)\n",
    "\n",
    "# Make the request\n",
    "calInfo = get_api(cal_url)\n",
    "\n",
    "# Put the data into a pandas dataframe, sorted by calibration date and coefficient name\n",
    "columns = [\"uid\", \"calCoef\", \"calDate\", \"value\", \"calFile\"]\n",
    "instrumentCals = pd.DataFrame(columns=columns)\n",
    "for c in calInfo[\"calibration\"]:\n",
    "    for cc in c[\"calData\"]:\n",
    "        instrumentCals = instrumentCals.append({\n",
    "            \"uid\": cc[\"assetUid\"],\n",
    "            \"calCoef\": cc[\"eventName\"],\n",
    "            \"calDate\": convert_time(cc[\"eventStartTime\"]),\n",
    "            \"value\": cc[\"value\"],\n",
    "            \"calFile\": cc[\"dataSource\"]\n",
    "        }, ignore_index=True)\n",
    "instrumentCals.sort_values(by=[\"calDate\", \"calCoef\"], inplace=True)\n",
    "instrumentCals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ad9b4",
   "metadata": {},
   "source": [
    "Similiarly, there is an included function ```get_calibrations_by_uid```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_calibrations_by_uid(\"CGINS-DOSTAD-00228\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6288f492",
   "metadata": {},
   "source": [
    "---\n",
    "## Metadata\n",
    "Next, we can query OOINet for the metadata associated with the selected reference designator. The metadata contains such valuable information such as the available **methods** and **streams** (which are required to download the data), the **particleKeys** (the data variable names), and the associated **units**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c1ea5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metadata = get_metadata(refdes)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02ea76",
   "metadata": {},
   "source": [
    "Now, there are a lot of different variables returned in the metadata for the Oxygen sensor. Unless we want to reprocess the raw data ourselves, we really just want the scientifically relevant parameters. But which ones are those?\n",
    "\n",
    "Level 1, or L1 Data Products, are derived from L0 data, and provide data that has been calibrated using vendor-provided values or values derived from pre-deployment procedures, and that is in scientific units.\n",
    "\n",
    "Example: Data from Aanderaa oxygen (DOSTA) sensors are converted from the L0 data  (DCONCS_L0) to the L1 dissolved oxygen concentrations data product (DCONCS_L1) internally using the manufacturer’s conversion factors. While this is done onboard of the oxygen optode, the L0 products, such as the amplitude, phase, etc are available if desired.\n",
    "\n",
    "Level 2, or L2 Data Products are derived quantities created via an algorithm that draws on multiple L1 Data Products. L2 data products may be based on data from the same or a combination of separate instruments. \n",
    "Example: Level 1 temperature (TEMPWAT_L1) and salinity (PRACSAL_L1) data products are used in conjunction with the Level 1 nitrate concentration data product to produce a temperature and salinity corrected Level 2 dissolved oxygen concentration data product (DOXYGEN_L2).  \n",
    "\n",
    "**We recommend that end users work with Level 2 Data Products for analysis**, and use Level 0 and Level 1 products only in cases where the end user has a specific reason requiring these earlier-stage data products for their own data processing needs.\n",
    "\n",
    "We can query for the relevant data product level from the \"Preload Database\" using the **pdId** field from the returned metadata. One note of caution: the data levels for some variables such as **time** are set as **None** even though time is a fundamental parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets query \"time\" for its data level\n",
    "preload_url = URLS[\"preload\"] + \"/\" + \"7\"\n",
    "print(\"API Endpoint: \" + preload_url)\n",
    "\n",
    "get_api(preload_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25780aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the dissolved oxygen\n",
    "preload_url = URLS[\"preload\"] + \"/\" + \"14\"\n",
    "print(\"API Endpoint: \" + preload_url)\n",
    "\n",
    "get_api(preload_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddead3a",
   "metadata": {},
   "source": [
    "From above, we can see that **time** does not have a data level, whereas **dissolved_oxygen** is a **L2** level product. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6780ee9b",
   "metadata": {},
   "source": [
    "The function ```get_parameter_data_levels``` will take in the metadata you requested and return the relevant data levels for the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_levels = get_parameter_data_levels(metadata)\n",
    "data_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc3d33",
   "metadata": {},
   "source": [
    "With the returned parameter IDs, we can now filter for the L1 and L2 data levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca70ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_science_parameters(metadata, data_levels):\n",
    "    \"\"\"This function returns the science parameters for each datastream\"\"\"\n",
    "    \n",
    "    def filter_parameter_ids(pdId, pid_dict):\n",
    "        data_level = pid_dict.get(pdId)\n",
    "        if data_level is not None:\n",
    "            if data_level > 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # Filter the parameters for processed science parameters\n",
    "    mask = metadata[\"pdId\"].apply(lambda x: filter_parameter_ids(x, data_levels))\n",
    "    metadata = metadata[mask]\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deba538",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_variables = filter_science_parameters(metadata, data_levels)\n",
    "science_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3921a",
   "metadata": {},
   "source": [
    "Now, we can notice that the timestamps, etc did were filtered out. This is because they don't have a defined data product level. An additional wrinkle is that **```time```** is NOT the default dimension of delivered netCDF files - this means it needs to be specifically requested for data requests. This is something to be aware of when requesting only specific data variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb88d3",
   "metadata": {},
   "source": [
    "---\n",
    "## Requesting Data\n",
    "\n",
    "### Synchronous Data\n",
    "The fast(er) way to get data is via a **Synchronous** request. A synchronous request can accept the following specifications:\n",
    "* limit (required): specifies number of data points with a maximum of 20000 \n",
    "* beginDT (optional): start date as YYYY-mm-ddTHH:MM:SS.fffZ format\n",
    "* endDT (optional): end date in same format as beginDT\n",
    "* parameters (optional): numeric IDs of which parameters to get \n",
    "\n",
    "If you do not specify the limit, the request defaults to an **asynchronous** request which is covered below.\n",
    "\n",
    "We'll go through the steps to narrow down to get just the oxygen data for the year 2015 from the Global Ocean Station Papa Flanking Mooring A oxygen sensor. The request we would build would have the following specifications:\n",
    "* limit: 20000\n",
    "* beginDT: 2015-01-01T00:00:00.000Z\n",
    "* endDT: 2016-01-01T00:00:00.000Z\n",
    "* parameters: 7 (time), 14 (dissolved oxygen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"recovered_host\"\n",
    "stream = \"dosta_abcdjm_sio_instrument_recovered\"\n",
    "\n",
    "# Request the oxygen data from the \n",
    "data_url = \"/\".join((URLS[\"data\"], site, node, sensor, method, stream))\n",
    "\n",
    "params = {\n",
    "    \"beginDT\": \"2015-01-01T00:00:01.000Z\",\n",
    "    \"endDT\": \"2016-01-01T00:00:01.000Z\",\n",
    "    \"limit\": \"20000\"\n",
    "}\n",
    "\n",
    "data = get_api(data_url, params)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd90f72",
   "metadata": {},
   "source": [
    "We'll put the data, which is in a JSON object, into a pandas dataframe for easier reading and parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b07073",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e293593",
   "metadata": {},
   "source": [
    "Query all of the different column names which are the different parameters returned in the data request we just made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6406cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d74697",
   "metadata": {},
   "source": [
    "That is a lot of data that was returned that we aren't necessarily interested in. Let's narrow our request to just the **dissolved_oxygen**, which from above we know has a parameter ID of PD14:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e8385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select the method and stream\n",
    "method = \"recovered_host\"\n",
    "stream = \"dosta_abcdjm_sio_instrument_recovered\"\n",
    "\n",
    "# Build the base url to request the data\n",
    "data_url = \"/\".join((URLS[\"data\"], site, node, sensor, method, stream))\n",
    "\n",
    "# Add in the limits to the data request\n",
    "params = {\n",
    "    \"beginDT\": \"2015-01-01T00:00:01.000Z\",\n",
    "    \"endDT\": \"2016-01-01T00:00:01.000Z\",\n",
    "    \"limit\": \"20000\",\n",
    "    \"parameters\": \"14\"\n",
    "}\n",
    "\n",
    "# Request the data\n",
    "data = get_api(data_url, params)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b952482e",
   "metadata": {},
   "source": [
    "Put the data, which is in a JSON object, into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e4539",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586b58d0",
   "metadata": {},
   "source": [
    "We requested just the dissolved oxygen data, but we also got **ctdmo_ghqr_sio_mule_instrument-ctdmo_seawater_temperature** and **ctdmo_ghqr_sio_mule_instrument-practical_salinity** even though we didn't request them. So what are they? They are the seawater temperature and practical salinity needed to calculate the dissolved oxygen concentration from the DO measured by the instrument. The part before **-** in the names tell you the data stream the parameter comes from. \n",
    "\n",
    "However, this time we don't have any **time** parameter! That's because we forgot to request it. So let's try this once more, including the **time** parameter id of **7**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa590950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the method and stream\n",
    "method = \"recovered_host\"\n",
    "stream = \"dosta_abcdjm_sio_instrument_recovered\"\n",
    "\n",
    "# Build the base url to request the data\n",
    "data_url = \"/\".join((URLS[\"data\"], site, node, sensor, method, stream))\n",
    "\n",
    "# Add in the limits to the data request, this time remembering to include \"time\" parameter \"7\"\n",
    "params = {\n",
    "    \"beginDT\": \"2015-01-01T00:00:01.000Z\",\n",
    "    \"endDT\": \"2016-01-01T00:00:01.000Z\",\n",
    "    \"limit\": \"20000\",\n",
    "    \"parameters\": \"7,14\"\n",
    "}\n",
    "\n",
    "# Request the data\n",
    "data = get_api(data_url, params)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80e97c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Put the data into a dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the time stamps from a string into a datetime object\n",
    "df[\"time\"] = df[\"time\"].apply(lambda x: ntp_seconds_to_datetime(x))\n",
    "\n",
    "# Get the deployment number from the \"pk\" dictionary\n",
    "df[\"deployment\"] = df[\"pk\"].apply(lambda x: int(x[\"deployment\"]))    \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b1612",
   "metadata": {},
   "source": [
    "Now we have everything we could want in order to begin visualizing and analyzing the oxygen data we requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8a633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cefa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(21,15))\n",
    "\n",
    "ax[0].plot(df[\"time\"], df[\"dissolved_oxygen\"], marker=\".\", linestyle=\"\", color=\"tab:blue\")\n",
    "ax[0].set_ylabel(\"Dissolved Oxygen \\n[umol/kg]\", fontsize=16, weight=\"bold\")\n",
    "ax[0].set_ylim((250, 400))\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot(df[\"time\"], df[\"ctdmo_ghqr_sio_mule_instrument-ctdmo_seawater_temperature\"], marker=\".\", linestyle=\"\", color=\"tab:red\")\n",
    "ax[1].set_ylabel(\"Seawater Temperature \\n[deg_C]\", fontsize=16, weight=\"bold\")\n",
    "ax[1].grid()\n",
    "\n",
    "ax[2].plot(df[\"time\"], df[\"ctdmo_ghqr_sio_mule_instrument-practical_salinity\"], marker=\".\", linestyle=\"\", color=\"tab:green\")\n",
    "ax[2].set_ylabel(\"Practical Salinity\", fontsize=16, weight=\"bold\")\n",
    "ax[2].set_ylim((31.5, 33.5))\n",
    "ax[2].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b911f4d",
   "metadata": {},
   "source": [
    "Looking at the data, there seems to be a lot of noise in the oxygen data from the first 5 months of 2015, while there is a lot of noise from June - October in the salinity and temperature data. Wonder what could be going on? The first place to check are the **annotations**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a2ad2",
   "metadata": {},
   "source": [
    "---\n",
    "## Annotations\n",
    "Annotations are technical notes or qualitative data assessments of the instrument added by staff from the institutions operating the sensors. They represent the first human-in-the-loop (HITL) quality control review of the data coming from the sensor, and may contain important information about the state of the instrument, such as the presence of biofouling, power issues, communications disruptions, and other such issues. \n",
    "\n",
    "Annotations are ideal for removing known and identified bad data from a dataset before further processing. While it is not within the purview of OOI to comprehensively flag all such issues, such that additional end user QA/QC is required, existing annotations will provide valuable and time-saving information to support end user analysis.\n",
    "\n",
    "An annotation downloaded from the OOI Data Portal is associated with a particular reference designator and not an individual instrument. It may also be further limited to a particular stream for a given reference designator, such as the pco2w_abc_dcl_instrument_recovered for recovered data from the SAMI-pCO2 instrument, as well as further limited to particular parameters. Annotations are either open-ended, with a start time (beginDT) and no end time (endDT), or may have both a start and end time. Times are returned in unix epoch microseconds. Lastly, a qcFlag may be assigned to a particular annotation following the QARTOD flagging conventions described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59377aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The annotation endpoint is: {URLS['anno']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb7e24",
   "metadata": {},
   "source": [
    "Now construct the annotation request. Once we get the annotations, we have to convert the beginDT and endDT from the unix epoch time milliseconds into a readable time stamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10355691",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"beginDT\": \"2015-01-01T00:00:01.000Z\",\n",
    "    \"endDT\": \"2016-01-01T00:00:01.000Z\",\n",
    "}\n",
    "\n",
    "annotations = get_annotations(refdes, beginDT=\"2015-01-01T00:00:01.000Z\", endDT=\"2016-01-01T00:00:01.000Z\")\n",
    "annotations[\"beginDT\"] = annotations[\"beginDT\"].apply(lambda x: convert_time(x))\n",
    "annotations[\"endDT\"] = annotations[\"endDT\"].apply(lambda x: convert_time(x))\n",
    "annotations\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b5be89",
   "metadata": {},
   "source": [
    "Print out the annotation text for each row in the table above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3b491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index in annotations.index:\n",
    "    start, stop, anno = annotations.loc[index, \"beginDT\"], annotations.loc[index, \"endDT\"], annotations.loc[index, \"annotation\"]\n",
    "    print(f\"{start} to {stop}: {anno}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a204d2",
   "metadata": {},
   "source": [
    "So now the noise in the oxygen data makes sense. There was biofouling! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e276d",
   "metadata": {},
   "source": [
    "---\n",
    "## Asynchronous Data Request\n",
    "\n",
    "Asynchronous data request are not limited in the number of data points that you can request. Additionally, they allow you to request **netCDF** and **csv** data formats as well as **JSON**. However, they are slower than synchronous data requests and, depending on the dataset, can be very very large. The available request specificiations include:\n",
    "* limit (required): if not specified, defaults netCDF \n",
    "* beginDT (optional): start date as YYYY-mm-ddTHH:MM:SS.fffZ format\n",
    "* endDT (optional): end date in same format as beginDT\n",
    "* parameters (optional): numeric IDs of which parameters to get \n",
    "* include_provenance (optional, default False): include a provenance file which specifies data processing paths\n",
    "* include_annotations (optional, default False): include a file with data annotations \n",
    "\n",
    "For the example we walked through above with the synchronous request, we can similarly request the asynchronous version to get netCDF datasets. Our specifications will be:\n",
    "* beginDT: 2015-01-01T00:00:00.000Z\n",
    "* endDT: 2016-01-01T00:00:00.000Z\n",
    "* parameters: 7 (time), 14 (dissolved oxygen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a22c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous \n",
    "data_url = \"/\".join((URLS[\"data\"], site, node, sensor, method, stream))\n",
    "\n",
    "params = {\n",
    "    \"beginDT\": \"2015-01-01T00:00:01.000Z\",\n",
    "    \"endDT\": \"2016-01-01T00:00:01.000Z\",\n",
    "    \"parameters\": \"7,14\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e8d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from halo import HaloNotebook\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356769c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with HaloNotebook(text=\"Waiting for request to process\", spinner=\"clock\"):\n",
    "    # Get the urls\n",
    "    urls = get_api(data_url, params=params)\n",
    "    # Check the status of the dataset preparation\n",
    "    status_url = [url for url in urls[\"allURLs\"] if re.match(r'.*async_results.*', url)][0]\n",
    "    status_url = status_url + \"/status.txt\"\n",
    "    status = SESSION.get(status_url)\n",
    "    # Hold until the dataset construction is finished\n",
    "    while status.status_code != requests.codes.ok:\n",
    "        time.sleep(2)\n",
    "        status = SESSION.get(status_url)\n",
    "        \n",
    "# Now fetch the thredds_url from the \n",
    "for d in urls['allURLs']:\n",
    "    if 'thredds' in d:\n",
    "        thredds_url = d\n",
    "\n",
    "thredds_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681dafa",
   "metadata": {},
   "source": [
    "Now, the url we want is the \"thredds catalog\" in the 'allURLs' dictionary entry above. We have to parse out the catalog for the netCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef5d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(thredds_url).text\n",
    "soup = BeautifulSoup(page, \"html.parser\")\n",
    "pattern = re.compile('.*\\\\.nc$')\n",
    "catalog = sorted([node.get('href') for node in soup.find_all('a', text=pattern)])\n",
    "catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62332b79",
   "metadata": {},
   "source": [
    "However, notice that there are some datasets which are NOT oxygen data. These include CTDMOG040, which supplies the practical salinity and temperature data necessary for calculating the oxygen concentration, and a FLORT datastream. We'll parse those out of the catalog and leave us with just the DOSTA datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of the unwanted datasets\n",
    "catalog = [x for x in catalog if refdes in x.split(\"/\")[-1]] \n",
    "catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ff7e4",
   "metadata": {},
   "source": [
    "In order to download the data, we want to get the catalog files from the **fileServer** url. Then we can download the netCDF files to whatever directory we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0463b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e05c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download, we need the fileServer\n",
    "fileServer = URLS[\"fileServer\"]\n",
    "netCDF_files = [re.sub(\"catalog.html\\?dataset=\", fileServer, file) for file in catalog]\n",
    "\n",
    "# Make a save directory\n",
    "saveDir = f\"../data/{refdes}/\"\n",
    "if not os.path.exists(saveDir):\n",
    "    os.makedirs(saveDir)\n",
    "\n",
    "for file in netCDF_files:\n",
    "        filename = file.split(\"/\")[-1]\n",
    "        saveFile = \"/\".join((saveDir, filename))\n",
    "        print(f\"Saving {filename} to {saveFile} \\n\")\n",
    "        urlretrieve(file, saveFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf719a5",
   "metadata": {},
   "source": [
    "#### Packaged functions\n",
    "The steps outlined above have been simplified into several easier-to-use functions as part of the package with this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb55794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get the thredds_url\n",
    "thredds_url = get_thredds_url(refdes, method, stream, goldCopy=False, beginDT=params[\"beginDT\"], endDT=params[\"endDT\"], parameters=params[\"parameters\"])\n",
    "\n",
    "# Second, access the catalog\n",
    "catalog = get_thredds_catalog(thredds_url)\n",
    "\n",
    "# Next , get rid of the unwanted datasets\n",
    "catalog = [x for x in catalog if refdes in x.split(\"/\")[-1]] \n",
    "\n",
    "# Lastly, download the datasets\n",
    "saveDir = f\"../data/{refdes}/\"\n",
    "download_netCDF_files(catalog, goldCopy=False, saveDir=saveDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae53a197",
   "metadata": {},
   "source": [
    "There are several ways to open the data. You might be tempted to utilize ```xarray.open_mfdataset``` feature to open all of the datasets at once into a single file. This will fail because moorings are deployed such that the new mooring goes into the water before the old mooring is recovered, leading to overlapping time periods. The ```open_mfdataset``` function requires increasing primary dimension. This can be avoided by using a ```preprocess``` routine to trim the overlapping portions of the datasets, but then you potentially lose some valuable data when two instruments are in the water at the same time! \n",
    "\n",
    "Instead, we can concatentate the datasets together. However, this is recommended only with smaller datasets; long timeseries or large datasets, especially profilers, will cause you to run out of working memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89fe52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "netCDF_files = [\"/\".join((saveDir, x)) for x in sorted(os.listdir(saveDir))]\n",
    "for file in netCDF_files:\n",
    "    ds = xr.open_dataset(file)\n",
    "    ds = ds.swap_dims({\"obs\":\"time\"})\n",
    "    try:\n",
    "        new_ds = xr.concat([new_ds, ds], dim=\"time\")\n",
    "    except:\n",
    "        new_ds = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ba685",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = new_ds.sortby(\"time\")\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe523754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
